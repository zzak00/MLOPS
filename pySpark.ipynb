{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession introduced in version 2.0, It is an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD, DataFrame. It’s object spark is default available in pyspark-shell and it can be created programmatically using SparkSession. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# instantiate a spark Session\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulator : are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark RDD repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark coalesce() is used to only decrease the number of partitions in an efficient way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelize 6\n"
     ]
    }
   ],
   "source": [
    "# In RDD, you can create parallelism at the time of the creation of an RDD using parallelize(), textFile() and wholeTextFiles().\n",
    "rdd1 = spark.sparkContext.parallelize(list(range(0,20)),6)\n",
    "print(\"parallelize \" +str(rdd.getNumPartitions()))\n",
    "rdd1.saveAsTextFile(\"partition2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n",
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"re-partition2\")\n",
    "\n",
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"coalesce2\")\n",
    "\n",
    "# --> Key Difference : coalesce() is optimized or improved version of repartition() where the movement of the data across the partitions is lower using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0', 1: '1'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = spark.sparkContext.broadcast({0:'0',1:'1'})\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. <br>\n",
    "PySpark RDD Benefits: \n",
    "1. In-Memory Processing\n",
    "2. Immutability\n",
    "3. Fault Tolerance\n",
    "4. Lazy Evolution\n",
    "5. Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD transformations – Transformations are lazy operations, instead of updating an RDD, these operations return another RDD. Transformations are lazy meaning they don’t execute until you call an action on RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap()transformation flattens the RDD after applying the function and returns a new RDD. On the below example, first, it splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map() transformation is used the apply any complex operations like adding a column, updating a column e.t.c, the output of map transformations would always have the same number of records as input. <br>In our word count example, we are adding a new column with value 1 for each word, the result of the RDD is PairRDDFunctions which contains key-value pairs, word of type String as Key and 1 of type Int as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey() merges the values for each key with the function specified. <br> In our example, it reduces the word string by applying the sum function on value. The result of our RDD contains unique words and their count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortByKey() transformation is used to sort RDD elements on key.<br> In our example, first, we convert RDD[(String,Int]) to RDD[(Int, String]) using map transformation and apply sortByKey which ideally does sort on an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter() transformation is used to filter the records in an RDD. <br> In our example we are filtering all words starts with “a”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd6 = rdd3.filter(lambda x : 'a' in x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD actions – operations that trigger computation and return RDD values to a driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() – Returns the number of records in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 144\n"
     ]
    }
   ],
   "source": [
    "print(\"Count : \"+str(rdd6.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first() – Returns the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Record : 9,Project\n"
     ]
    }
   ],
   "source": [
    "firstRec = rdd5.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max() – Returns max record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Record : 27,with\n"
     ]
    }
   ],
   "source": [
    "datMax = rdd5.max()\n",
    "print(\"Max Record : \"+str(datMax[0]) + \",\"+ datMax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce() – Reduces the records to single, we can use this to count or sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataReduce Record : 522\n"
     ]
    }
   ],
   "source": [
    "# Action - reduce\n",
    "totalWordCount = rdd5.reduce(lambda a,b: (a[0]+b[0],a[1]+b[1]))\n",
    "print(\"dataReduce Record : \"+str(totalWordCount[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take() – Returns the record specified as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = rdd6.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wonderland', 1), ('Carroll', 1), ('anyone', 1)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair RDD is a key-value pair This is mostly used RDD type, \n",
    "1. ShuffledRDD \n",
    "2. DoubleRDD \n",
    "3. SequenceFileRDD  \n",
    "4. HadoopRDD \n",
    "5. ParallelCollectionRDD \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffling** is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like gropByKey(), reduceByKey(), join() on RDDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
